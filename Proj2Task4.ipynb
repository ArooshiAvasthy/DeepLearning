{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-MrlDTS3vFMO"
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Conv2D, MaxPooling2D, UpSampling2D,Flatten\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv2D, MaxPooling2D, BatchNormalization, Activation, Reshape, UpSampling2D,Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3F4af3YTxVVC"
   },
   "outputs": [],
   "source": [
    "seen_data_training = pd.read_csv('seen-dataset/seen-dataset/dataset_seen_training_siamese.csv')\n",
    "seen_data_validation= pd.read_csv('dataset_seen_validation_siamese.csv')\n",
    "\n",
    "shuffle_data_training =pd.read_csv('unseen-dataset/unseen-dataset/dataset_seen_training_siamese.csv')\n",
    "shuffle_data_validation= pd.read_csv('dataset_Shseen_validation_siamese.csv')\n",
    "\n",
    "unseen_data_training = pd.read_csv('dataset_unseen_training_siamese.csv')\n",
    "unseen_data_validation= pd.read_csv('dataset_unseen_validation_siamese.csv')\n",
    "\n",
    "\n",
    "seen_data_training=seen_data_training.drop('Unnamed: 0',axis=1)\n",
    "shuffle_data_training=shuffle_data_training.drop('Unnamed: 0',axis=1)\n",
    "unseen_training=unseen_data_training.drop('Unnamed: 0',axis=1)\n",
    "\n",
    "\n",
    "seen_data_validation=seen_data_validation.drop('Unnamed: 0',axis=1)\n",
    "shuffle_data_validation=shuffle_data_validation.drop('Unnamed: 0',axis=1)\n",
    "unseen_data_validation=unseen_data_validation.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YEa0AtzcxhyX"
   },
   "outputs": [],
   "source": [
    "l_train=seen_data_training['left']\n",
    "r_train=seen_data_training['right']\n",
    "target_seen=seen_data_training['label'].values.tolist()\n",
    "\n",
    "\n",
    "\n",
    "l_train_shuffle=shuffle_data_training['left']\n",
    "r_train_shuffle=shuffle_data_training['right']\n",
    "target_seen_shuffle=shuffle_data_training['label'].values.tolist()\n",
    "\n",
    "\n",
    "l_train_un=unseen_data_training['left']\n",
    "r_train_un=unseen_data_training['right']\n",
    "target_seen_un=unseen_data_training['label'].values.tolist()\n",
    "\n",
    "l_train1=seen_data_validation['left']\n",
    "r_train1=seen_data_validation['right']\n",
    "target_seen1=seen_data_validation['label'].values.tolist()\n",
    "\n",
    "\n",
    "l_train1_shuffle=shuffle_data_validation['left']\n",
    "r_train1_shuffle=shuffle_data_validation['right']\n",
    "target_seen1_shuffle=shuffle_data_validation['label'].values.tolist()\n",
    "\n",
    "l_train1_un=unseen_data_validation['left']\n",
    "r_train1_un=unseen_data_validation['right']\n",
    "target_seen1_un=unseen_data_validation['label'].values.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2V1CY_l6xmOJ"
   },
   "outputs": [],
   "source": [
    "def train_data_proc(path,loop_l,loop_r,target):\n",
    "  import PIL\n",
    "  from scipy import misc\n",
    "  import imageio\n",
    "  count = 0\n",
    "  X_train=[]\n",
    "  for name in loop_l:\n",
    "#     print(count)\n",
    "    try:\n",
    "      X_train.append(np.array(PIL.Image.open(path+\"/\"+name).convert(\"L\")))\n",
    "      count += 1\n",
    "    except:\n",
    "  #     del target_seen[count]\n",
    "        pass\n",
    "  \n",
    "  print(\"something done\")\n",
    "  count=0\n",
    "  X_train1=[]\n",
    "  for name in loop_r:\n",
    "  #   print(count)\n",
    "      try:\n",
    "        X_train1.append(np.array(PIL.Image.open(path+\"/\"+name).convert(\"L\")))\n",
    "        count += 1\n",
    "      except Exception as e:\n",
    "        \n",
    "      # Remove count index in target_seen\n",
    "        del target[count]\n",
    "      # Remove count index in left_target\n",
    "        del X_train[count]\n",
    "        \n",
    "  \n",
    "  return X_train,X_train1\n",
    "\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdwaUfcKxr8J"
   },
   "outputs": [],
   "source": [
    "X_train,X_train1=train_data_proc(\"seen-dataset/seen-dataset/TrainingSet\",l_train,r_train,target_seen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZP4G4iPHytDY"
   },
   "outputs": [],
   "source": [
    "def valid_data_proc(path,loop_l,loop_r,target):\n",
    "  import PIL\n",
    "  from scipy import misc\n",
    "  import imageio\n",
    "  X_train_v=[]\n",
    "  for name in loop_l:\n",
    "    X_train_v.append(np.array(PIL.Image.open(path+\"/\"+name).convert(\"L\")))\n",
    "  \n",
    "  count=0\n",
    "  X_train1_v=[]\n",
    "  for name in loop_r:\n",
    "  #   print(count)\n",
    "    try:\n",
    "      X_train1_v.append(np.array(PIL.Image.open(path+\"/\"+name).convert(\"L\")))\n",
    "      count += 1\n",
    "    except Exception as e:\n",
    "      # Remove count index in target_seen\n",
    "      del target[count]\n",
    "    # Remove count index in left_target\n",
    "      del X_train_v[count]\n",
    "      \n",
    "  return X_train_v , X_train1_v \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "erJHo0Aey9-H"
   },
   "outputs": [],
   "source": [
    "X_train,X_train1=train_data_proc(\"seen-dataset/seen-dataset/TrainingSet\",l_train,r_train,target_seen)\n",
    "#print(X_train)\n",
    "X_train_sh,X_train1_sh=train_data_proc(\"shuffled-dataset/shuffled-dataset/TrainingSet\",l_train_shuffle,r_train_shuffle,target_seen_shuffle)\n",
    "X_train_un,X_train1_un=train_data_proc(\"unseen-dataset/unseen-dataset/TrainingSet\",l_train_un,r_train_un,target_seen_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_v , X_train1_v=valid_data_proc(\"seen-dataset/seen-dataset/ValidationSet\",l_train1,r_train1,target_seen1)\n",
    "X_train_v_sh , X_train1_v_sh=valid_data_proc(\"shuffled-dataset/shuffled-dataset/ValidationSet\",l_train1_shuffle,r_train1_shuffle,target_seen1_shuffle)\n",
    "X_train_v_un , X_train1_v_un=valid_data_proc(\"unseen-dataset/unseen-dataset/ValidationSet\",l_train1_un,r_train1_un,target_seen1_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q1HA3xkSzTeH"
   },
   "outputs": [],
   "source": [
    "X_train=np.array(X_train)\n",
    "X_train1=np.array(X_train1)\n",
    "Y_train=np.array(target_seen)\n",
    "\n",
    "\n",
    "X_train_un=np.array(X_train_un)\n",
    "X_train1_un=np.array(X_train1_un)\n",
    "Y_train_un=np.array(target_seen_un)\n",
    "\n",
    "X_train_shuffle=np.array(X_train_sh)\n",
    "X_train1_shuffle=np.array(X_train1_sh)\n",
    "Y_train_shuffle=np.array(target_seen_shuffle)\n",
    "\n",
    "X_train_v=np.array(X_train_v)\n",
    "X_train1_v=np.array(X_train1_v)\n",
    "Y_train1=np.array(target_seen1)\n",
    "\n",
    "\n",
    "X_train_v_un=np.array(X_train_v_un)\n",
    "X_train1_v_un=np.array(X_train1_v_un)\n",
    "Y_train1_un=np.array(target_seen1_un)\n",
    "\n",
    "X_train_v_shuffle=np.array(X_train_v_sh)\n",
    "X_train1_v_shuffle=np.array(X_train1_v_sh)\n",
    "Y_train1_shuffle=np.array(target_seen1_shuffle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WckflUurwN2O"
   },
   "outputs": [],
   "source": [
    "input_img = Input(shape=(64, 64, 1)) \n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
    "encoded = Flatten()(x)\n",
    "encoded = Dense(8*8*8, activation='relu', name='latent')(encoded)\n",
    "\n",
    "r = Reshape(target_shape=(8,8,8))(encoded)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(r)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same',name='output')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CMBJpq52zKCc"
   },
   "outputs": [],
   "source": [
    "encoder = Model(autoencoder.inputs, autoencoder.get_layer('latent').output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fGoFUB9KwmYo"
   },
   "outputs": [],
   "source": [
    "autoencoder.fit(X_train.reshape(len(X_train),64,64,1), X_train1.reshape(len(X_train1),64,64,1),\n",
    "                epochs=10,\n",
    "                batch_size=128,\n",
    "                shuffle=True,\n",
    "                validation_data=(X_train_v.reshape(len(X_train_v),64,64,1), X_train1_v.reshape(len(X_train1_v),64,64,1)),\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HdMFAMbbxD6D"
   },
   "outputs": [],
   "source": [
    "preds=autoencoder.predict((X_train_v.reshape(len(X_train_v),64,64,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qr9qI9ZV3yqs"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7nPIpIem85In"
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "display(csv_data.head()), display(csv_data.shape)\n",
    "train_path = 'seen-dataset/seen-dataset/TrainingSet/'\n",
    "train_images = os.listdir(train_path)\n",
    "\n",
    "val_path = 'seen-dataset/seen-dataset/ValidationSet/'\n",
    "val_images = os.listdir(val_path)\n",
    "len(train_images)+len(val_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cEjLlFi9OeB"
   },
   "outputs": [],
   "source": [
    "dataset_columns = csv_data.columns\n",
    "csv_data['imagename'] = csv_data['imagename'].astype('str')\n",
    "display(csv_data.head(20))\n",
    "csv_data.shape\n",
    "f = []\n",
    "for col in dataset_columns[1:]:\n",
    "    f.append(csv_data[col].unique().shape[0])\n",
    "f,len(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xfaosnVf9O4y"
   },
   "outputs": [],
   "source": [
    "feature_lengths = {}\n",
    "for col in dataset_columns[1:]:\n",
    "    feature_lengths[col] = len(csv_data[col].unique())\n",
    "feature_lengths = list(feature_lengths.values())\n",
    "feature_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "anRu6Giq3b6z"
   },
   "outputs": [],
   "source": [
    "input_layer = Input((64,64,1))\n",
    "model = BatchNormalization()(input_layer)\n",
    "\n",
    "model = Conv2D(activation='relu',filters=32, kernel_size=(32,32), padding='same')(model)\n",
    "model = Conv2D(activation='relu',filters=64, kernel_size=(32,32), strides=(1,1), padding='same')(model)\n",
    "model = MaxPooling2D((2,2))(model)\n",
    "model = Dropout(0.25)(model)\n",
    "\n",
    "model = Conv2D(activation='relu',filters=128, kernel_size=(16,16), padding='same')(model)\n",
    "model = Conv2D(activation='relu',filters=256, kernel_size=(16,16), strides=(1,1), padding='same')(model)\n",
    "model = MaxPooling2D((2,2))(model)\n",
    "model = Dropout(0.25)(model)\n",
    "\n",
    "model = Conv2D(activation='relu',filters=128, kernel_size=(8,8), padding='same')(model)\n",
    "model = Conv2D(activation='relu',filters=256, kernel_size=(8,8), strides=(1,1), padding='same')(model)\n",
    "model = MaxPooling2D((2,2))(model)\n",
    "model = Dropout(0.25)(model)\n",
    "\n",
    "\n",
    "dense_layer = Dense(512, activation='relu')(model)\n",
    "dense_layer = Dropout(0.5)(dense_layer)\n",
    "out_dense_layer=[]\n",
    "for i in range(1,len(f)+1):\n",
    "    out_dense_layer.append(Dense(f[i-1] , activation='softmax', name = 'out_feature_'+str(i))(Flatten()(Dense(64 , activation='relu', name = 'dense_layer_'+str(i))(dense_layer))))\n",
    "\n",
    "human_features = Model(inputs=[input_layer], outputs=out_dense_layer)\n",
    "human_features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZbiXnFWE98Sf"
   },
   "outputs": [],
   "source": [
    "losses = {}\n",
    "lossWeights = {}\n",
    "for i in range(1,16):\n",
    "    losses[\"out_feature_\"+str(i)] = \"categorical_crossentropy\"\n",
    "    lossWeights[\"out_feature_\"+str(i)] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = ModelCheckpoint(filepath='15_human_features_weights_unseen_v1.h5', monitor='val_loss',period=1,save_best_only=True,save_weights_only=True,mode='auto',verbose=3)\n",
    "es = EarlyStopping(patience=10000, monitor='val_loss', min_delta=0.0005, mode='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8mbyFvIW2wBR"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adadelta, SGD, Adam\n",
    "EPOCHS = 10\n",
    "INIT_LR = 0.0001\n",
    "opt = Adadelta(lr=INIT_LR, decay=1e-04)\n",
    "human_features.compile(optimizer=opt, loss=losses, loss_weights=lossWeights,metrics=[\"accuracy\"])\n",
    "human_features.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzzc4aK_AzcY"
   },
   "outputs": [],
   "source": [
    "tg = (n for n in list(preds))\n",
    "vg=(n for n in list(X_train_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvH2aK89-DLK"
   },
   "outputs": [],
   "source": [
    "\n",
    "hist = human_features.fit(preds,epochs=EPOCHS\n",
    "                                    , steps_per_epoch = 1\n",
    "                                    , validation_data = X_train_v\n",
    "                                    , validation_steps = 1\n",
    "                                    , verbose = 3\n",
    "                                    \n",
    "                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hist = pd.DataFrame(hist.history)\n",
    "df_hist.plot(subplots=True, figsize=(15,100))\n",
    "print(df_hist['val_loss'].min())\n",
    "df_hist[df_hist['val_loss']<9.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pY3mn_P6AUwv"
   },
   "outputs": [],
   "source": [
    "\n",
    "# human_features.load_weights('human_features_weights_v2.h5')\n",
    "human_features.load_weights('15_human_features_weights_unseen_v1.h5')\n",
    "vx,vo=next(vg)\n",
    "# print(len(vo))\n",
    "fig = plt.figure(figsize=(80,200))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=1.51, wspace=0.05)\n",
    "for i in range(64):\n",
    "#     print(i, '\\nPredicted:')\n",
    "    \n",
    "    real=''\n",
    "    for f in range(15):\n",
    "#         print(vo[i][f])\n",
    "        real+=str(np.argmax(vo[f][i])+1)+','\n",
    "    \n",
    "    f_probs = human_features.predict(np.expand_dims(vx[i],0))\n",
    "#     print(f_probs)\n",
    "    pred=''\n",
    "    for prob in f_probs:\n",
    "#         print(prob)\n",
    "        pred+=str(np.argmax(prob[0])+1)+','\n",
    "    \n",
    "    ax = fig.add_subplot(64, 1, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(vx[i].reshape((64,64)))\n",
    "    ax.set_title('real:'+real+'\\n pred:'+pred)\n",
    "#     print('\\n----------')\n",
    "#     print('True:')\n",
    "#     for op in o:\n",
    "#         print(np.argmax(op[i]),end=',')\n",
    "        \n",
    "#     print('\\n++++++',i,'++++++')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,30))\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "for i in range(30):\n",
    "    ax = fig.add_subplot(1, 30, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(inputs[64-30:64][i].reshape((64,64)))\n",
    "    ax.set_title(i+34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(inputs[10].reshape((64,64)))\n",
    "plt.subplots_adjust(hspace = 0.1)\n",
    "for i in range(1,9):\n",
    "    visualize_model = Model(human_features.input, human_features.get_layer('dense_layer_'+str(i)).output)\n",
    "    vis_out = visualize_model.predict(np.expand_dims(inputs[10],0))[0]\n",
    "    vis_out = np.rollaxis(a=vis_out, start=0, axis=2)\n",
    "    f,ax = plt.subplots(8,8)\n",
    "    f.subplots_adjust(hspace = .00, wspace=.02, top=0.95)\n",
    "    f.set_size_inches(15,15)\n",
    "    f.suptitle('dense_layer_'+str(i), fontsize=15)\n",
    "#     f.subplots_adjust(hspace = 0.1)\n",
    "#     plt.title('dense_layer_'+str(i))\n",
    "    print(vis_out.shape)\n",
    "    for r in range(8):\n",
    "        for c in range (8):\n",
    "            ax[r,c].imshow(vis_out[c+r*8])\n",
    "            ax[r,c].axis('off')\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_g = datagen(len(X_train_v)*5,vshift=(-10,10), list_of_writers=X_train_v)\n",
    "test_x,test_y = next(test_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=human_features_eval.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y=np.array(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y[:,0,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_im_normal = test_x[0]#255.0-cv2.imread('../Handwriting/cleaned_AND_data_64x64/0301c_num2.png',0)\n",
    "test_im_shifted = test_x[1]#255.0-cv2.imread('../Handwriting/cleaned_AND_data_64x64/0301c_num1.png',0)\n",
    "f,ax = plt.subplots(1,2)\n",
    "f.subplots_adjust(hspace = .00, wspace=.05)\n",
    "f.set_size_inches(5,1*3)\n",
    "ax[0].imshow(test_im_normal.reshape((64,64)))\n",
    "ax[0].axis('off')\n",
    "\n",
    "ax[1].imshow(test_im_shifted.reshape((64,64)))\n",
    "ax[1].axis('off')\n",
    "ax[0].set_title('Left Input')\n",
    "ax[1].set_title('Right Input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "l = pred_y[:,0,:]\n",
    "r = pred_y[:,1,:]\n",
    "\n",
    "features = csv_data.keys()[1:]\n",
    "feature_similarity_score=cosine_similarity(l,r).diagonal()\n",
    "df_feat_sim_score = pd.DataFrame(columns=['features','feature_similarity_score'])\n",
    "df_feat_sim_score['features']=features\n",
    "df_feat_sim_score['feature_similarity_score']=feature_similarity_score\n",
    "df_feat_sim_score.plot.bar(x='features')\n",
    "df_feat_sim_score['feature_similarity_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_preds = human_features.predict(test_x[:2])for i in range(len(test_x[:2])):\n",
    "    for f in range(15):\n",
    "        print(np.argmax(class_preds[f][i]),end=',')\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data.var()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled25.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
